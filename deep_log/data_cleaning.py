
import json






def remove_header(lines):
    '''
    Remove headers from.txt file
    :param lines: list
    :return: list with the logs without the headers
    '''
    lines = lines[5:]

    return lines

import re


def clean_book(lines):
    clean_log = []
    time_log = []
    lines = remove_header(lines)
    time_pattern = r"\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}-\d{2}:\d{2}"
    time_pattern1 = r"\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}.\d{6}Z"
    ascii_pattern = r"\\u0009|\\n"
    colch = r"\{|\}"
    aspas = r"\"|\""
    for i in lines:
        i = re.sub(time_pattern, '', i)
        i = re.sub(time_pattern1, '', i)
        i = re.sub(ascii_pattern, ' ', i)
        #i = re.sub(colch, '', i)
        #i = i.split('"stream"')[0]
        #i = re.sub(aspas, '', i)
        i = i.replace('\t', '')

        to_json = json.loads(i)
        del to_json['stream']
        time = to_json.pop('time')
        key = to_json['log'].lstrip(' ')

        clean_log.append(key)
        time_log.append(time)

    return clean_log, time_log


def clean_sock(lines):
    '''
    Remove unimportant information from the logs lines before parse them using Drain3
    :param lines: list, lines of logs
    :return: clean_log, list with the cleaned log lines
    :return: time_log, list with the time in which the logs were generated

    '''
    clean_log = []
    time_log = []

    lines = remove_header(lines)
    time_pattern = r"\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}.\d{6}Z"
    ascii_pattern = r"\\n"
    for i in lines:
        i = re.sub(time_pattern, '', i)
        i = re.sub(ascii_pattern, ' ', i)

        #i = re.sub(colch, '', i)
        #i = i.split('"stream"')[0]
        #i = re.sub(aspas, '', i)
        i = i.replace('\t', '')

        to_json = json.loads(i)
        del to_json['stream']
        time = to_json.pop('time')
        key = to_json['log']


        clean_log.append(key)
        time_log.append(time)


    return clean_log, time_log


def write_logs(cluster_list, archive_path):
    '''
    Receives the cluster list of logs, generated by the log parser
    and writes on a .txt file named with archive_path

    :param cluster_list: list,
    :param archive_path: str, path in which to save the .txt file
    :return: None
    '''

    with open(f'{archive_path}', 'a') as f:
        for cluster_id in cluster_list:
            log = str(cluster_id)
            f.write(log)
            f.write(' ')



def read_logs(archive_path):
    '''
    Read .txt file with logs
    :param archive_path: str, path to the .txt file
    :return: list with one per entry
    '''
    with open(f'{archive_path}') as f:
        lines = f.readlines()
        lines = [i.replace('\n', '') for i in lines]


    return lines


